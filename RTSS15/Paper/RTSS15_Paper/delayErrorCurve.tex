\section{The Error-Delay Curve}
\label{delayErrorCurve}

In Section \ref{controlProblem}, we postulated the existence of an Estimation Error vs Computation Delay curve (the `error-delay curve').
This curve is used at every time step by the controller to determine the operating point $(\delta, \epsilon)$ for the next time step.
In this section we demonstrate how such a curve may be obtained for particular applications. 
Specifically, we consider a visual perception tool chain, where each tool of the tool chain has a knob that tunes its performance.
These knobs can be used to profile the tool chain and obtain its error-delay curve on particular platforms.

The first perception algorithm we consider is the FAST corner detector~\cite{rosten_2006_machine}.
Corners in an image can be tracked across video frames to perform self-localization by a moving robot. 
Thus it is important to detect corners at frame rate or faster, to provide a timely state estimate to the control algorithm.
The number $\#C$ of corners detected in a frame directly affects the runtime of the corner detector and the quality of the state estimate.
Generally speaking, detecting more corners requires a longer runtime, and results in better self-localization, \emph{assuming acceptable quality of the detected corners}.
If the additional corners are of poor quality (i.e., hard to track across frames), then we can expect the self-localization error to actually increase.
Thus the number $\#C$ of corners is a knob which can be varied to obtain an error-delay curve for self-localization with corner detection. 

Fig.~\ref{fig:fast} shows the error-delay curve of the FAST corner detector~\cite{rosten_2006_machine}.
The curve was obtained on an Odroid U3 [??], which is a quadcore \hatodoin{complete odroid specs}.
For each value of the knob $\#C$ (i.e., each requested number of corners), we ran the corner detector on a video sequence shot by a downward facing camera on-board a hexarotor while flying certain pre-set patterns.
The corners and some auxiliary data are then fed to a self-localization algorithm.
Ground truth for computing the self-localization error was obtained from the Vicon system which \hatodoin{half-sentence on what vicon does.}
As we repeat each flight several times, this results in a distribution of $(\delta,\epsilon)$ values for each value of $\#C$. 
We retained the $90^{th}$ percentile values for $\delta$ and $\epsilon$, since these are used as worst-case estimates by the controller of Section \ref{controlProblem}.
It can be seen that a larger number of requested corners produces a smaller estimation error and longer runtime.
Starting at ?? corners, the error increases, however.
We hypothesize this is due to the decreasing quality of the corners being returned by FAST.
\hatodoin{What if we use the Harris detector or something like that?}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/placeHolder}
\caption{Error-delay curve for the FAST corner detector running on the Odroid U3}
\label{fig:fast}
\end{figure}

Our second example is a more complete perception tool chain shown in Fig.~\ref{fig:chain}.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/placeHolder}
	\caption{Visual perception toolchain}
	\label{fig:chain}
\end{figure}

The tool chain takes in a video stream and tracks an Object Of Interest (OOI) across the frames.
The pixel classifier assigns to each pixel of the image (after potential pre-processing) the probability of its being a pixel of interest, i.e., of belonging to an OOI. 
A binary image is then obtained which assigns the value 1 to determined pixels of interest, and 0 to all others. 
A Connected Components (CC) algorithm is run on the binary image to segment its 1-valued pixels into disconnected objects.
A shape classifier is then run on each object to determine whether it is an object of interest or not.
The classifiers are machine learning algorithms that need to be trained on a training set before being used in the application.
In our implementation of this chain, the specific algorithms and their knobs are as follows:
the pixel and shape classifiers are Gaussian Mixture Models (GMMs), whose knob is the number of Gaussians in the mixture.
The connected components algorithm has a two-valued knob to choose between a 4-connected and 8-connected component implementation.
Thus the knob for the entire chain is $K$ = (\#Gaussians for pixel classifier, \#pixels for shape classifier, \#components), and has a total of $3 \times 3 \times 2 = 18$ values.
Fig.~\ref{fig:perceptionErrorVsDelay} shows the \emph{perception error}-delay curve.
The perception error is the difference between the estimated centroid of the OOI and the true centroid of the OOI.
For each point on the curve, first the classifiers were trained with the relevant knob values, and then run on the test set to obtain the $(\delta,\epsilon)$ values.
This curve can then be turned into an estimate error-delay curve by incorporating it into an estimation algorithm.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/placeHolder}
	\caption{Perception error vs Delay curve for the chain of Fig.~\ref{fig:chain}}
	\label{fig:perceptionErrorVsDelay}
\end{figure}

More generally, such curves can be obtained for a given state estimator by identifying the knobs that control the quality and runtime of the estimator, and profiling the estimator's code at the various values of the knobs.
\todo[inline]{Decision tree}
In effect, this gives us several Estimator tasks, each with a different utility. 
I.e., each such task strikes a different balance between computation time and quality of estimate.
At time step $k$, the controller schedules the task with the best trade-off for step $k+1$, in the sense of optimizing the cost function in Eq.??

It is worth noting that because power consumption is correlated to computation time, the error-delay curve can also be viewed as an error-power curve.
The power consumption of the estimator task can be modeled using the $\alpha$ term of Eq.??

