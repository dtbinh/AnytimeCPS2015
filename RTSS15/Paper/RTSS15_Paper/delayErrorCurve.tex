\section{Contract based perception algorithms}
\label{delayErrorCurve}

In Section \ref{sec:codesign}, we postulated the existence of an Estimation Error vs Computation Delay ($\delta,\epsilon$) curve. % (the `error-delay curve').
This curve is used at every time step by the controller to determine the operating point $(\delta,\epsilon)$ for the next time step.
In this section we demonstrate in detail how such a curve may be obtained for particular applications and how points along the curve are realized at runtime by the contract based perception algorithms.

\subsection{Profiling and composing an anytime contract based perception algorithm}

Recall, in section \ref{sec:codesign} we briefly discuss how a contract based perception tool chain can be obtained by composing different version of the individual run-to-completion algorithms used in the perception algorithm. The first stage towards this is to identify the different individual components of the perception tool chain and identifying how to realize different computation time and accuracy versions of them. We refer to these different realizations  knobs for the perception algorithm, and the control knobs may be as simple as changing the number of iterations in a loop \cite{greenMS} or finding alternate implementations with different resultant execution times and performance for the same functionality. As a running example in this section to illustrate our methods, we consider a perception tool chain for object recognition from camera images. An overview of the flow and individual components of the tool chain is shown in figure \ref{}. 

This tool chain takes in a video stream and tracks an Object Of Interest (OOI) across the frames. The first stage of it is a pixel classifier that assigns to each pixel of the image (after potential pre-processing) the probability of its being a pixel of interest, i.e., of belonging to an OOI or being a part of the background. A binary image is then obtained which assigns the value 1 to pixels of interest, and 0 to all others. A knob here in the pixel classification stage is to realize different computation time and performance profiles, is the complexity of the probabilistic model used to detect whether a pixel belongs to the object of interest. The knob in this case is whether we choose a Gaussian Mixture Model (GMM) with fewer components, which would be faster but less accurate, versus a GMM with more components which will take more computation time but provide better classification performance. Knobs where we over fit the training data are removed by cross-validation stage as is standard. Next, filtering and a Connected Components (CC) algorithm is run on the binary image to get rid of noise in the classification process and segment its 1-valued pixels into disconnected objects. A shape classifier is then run on each object to determine whether it is an object of interest or not. 
% the pixel classifier is a Gaussian Mixture Model (GMMs), whose knob is the number of Gaussians in the mixture.
In our implementation for the object detector that we use as the illustrative example here, the pixel classifier is a GMM based classifers with the number of knobs being the number of components in the GMM as discussed above. The filtering and connected components algorithm are lumped into one stage and have a two-valued knob to choose between a 4-connected and 8-connected component implementation. The shape classifier is also a GMM, but the knob for it is the number of shape features resulting in Gaussians in different dimensions being fit, and hence resulting in different computation time and classification accuracy.
% The number of Gaussian components for the shape classifier is fixed since we know in advance the number of objects in the training and test sets.
In the given setting the number of knob settings for the entire chain is $K$ = (\#Gaussians for pixel classifier, \#neighbors for CC, \#features for shape classifier), and has a total of $3 \times 2 \times 2 = 12$ values.

Note that for any given algorithm in the chain, the relation between knob value and quality of output is not necessarily monotonic. The pixel and shape classifiers are machine learning algorithms that need to be trained on a training set before being used and like all machine learning algorithms, their output quality for a given knob setting will depend on the actual data set. For the object detection algorithm we use for illustration in this section, we use images from the \textbf{<what?>} dataset where each image is of size 1200x1600 pixels. The same is a fortiori true of the quality of the output of the entire chain. This is also reflected in figure \ref{}, where the solid line shows the mean perception error (measured as the 2-norm of the x,y co-ordinates of the centroid of the object and the estimated centroid from the object recognition algorithm) and the $90^{th}$ percentile execution time for the different knob settings.

The second stage to composing a contract based perception algorithm is to again use the training data to profile all the possible combinations of knobs through an extensive training and validation phase. This profiling gives us: a) Not only the output quality (or accuracy) of the perception tool chain under consideration, but also b) probability distributions for execution times for the stages of the perception tool chain under different knob settings. Note, since intermediate qualities are not easily measurable or assignable for some of the blocks of the tool chain (e.g. Connected Components) and for many other algorithms in general, we assign quality (or accuracy) distributions to the realizations of the complete tool chain by composing together different knob settings. Note, this profiling stage is done at offline at design time and provides us information that we use at runtime.

\subsection{Decision tree based run-time execution of the contract based perception algorithm}

After the contract based perception algorithm has been composed and the execution time distributions of its individual components and the quality distributions from composing together various knob settings have been profiled, we can represent the offline profiled algorithm as a decision tree as shown in figure \ref{} for run-time decisions. This decision tree based representation, where edges represent individual functions for different knobs, e.g. GMM based pixel classifier with 3 components, edges have assigned to them the execution time distributions of the functions they represent. Also, paths from the top of the tree to a leaf represent one particular realization of the tool chain and have assigned to them distributions for accuracy of the algorithm. The decision of selecting which knob setting to use for a particular stage for a given criteria can now be posed as an optimization problem for edge selection in the decision tree. For example, consider the case where we want to minimize the perception error while meeting a time deadline with a given high probability, the problem can be written as \textbf{<insert equation here>}. This problem can be re-solved after each stage is executed, allowing the algorithm the flexibility to re-optimize its execution path (or knob settings) after obtaining updated information on time consumed in the past stages.

This decision tree based approach is the equivalent of selecting different versions of tasks (knobs for stages) and scheduling them in sequential order to best perform the object recognition task while maximizing utilization (or estimation accuracy) and meeting the given time contract or deadline with a high probability. Figure \ref{} shows the different task versions for each stage and the resulting schedule based on the knob settings for the stages.




Specifically, we consider two different visual perception tool chains, where each tool of the chain has a knob that tunes its performance.
These knobs can be used to profile the tool chain and obtain its error-delay curve on particular platforms.
Because power consumption is correlated to computation time, the error-delay curve can also be viewed as an error-power curve.
The power consumption of the estimator task can be included in the control cost function using the $\alpha$ term of Eq.??.
Thus, the controller can save power by selecting operating modes $(\delta,\epsilon)$ that achieve the control objectives at a lower energy cost.



% subsection
\input{fastCorner}
% subsection
%\input{objectRecognition}