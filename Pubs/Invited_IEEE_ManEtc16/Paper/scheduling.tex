%\subsection{Feedback driven scheduling and hardware mode selection}

The profiling results of the previous section indicate that the knobs $\sigma, F_c$ and $F_g$ allow us to trade-off throughput for power, as expected.
At runtime, we must decide which knob setting to choose at every time step. 
This is done by maximizing the following objective function at every time step $t$:

\begin{equation}
\max_{\sigma,F_{c},F_{g}} \alpha(t)\mathbf{\bar{T}}(\sigma,F_{c},F_{g}) + \frac{1-\alpha(t)}{\mathbf{E[\bar{P}]}(\sigma,F_{c},F_{g})}
\label{eq:cost_runtime}
\end{equation}

Recall that $\mathbf{\bar{T}}(\sigma,F_{c},F_{g})$ is the normalized throughput of the vanishing point algorithm under schedule $\sigma$ and with the CPU and GPU at frequencies $F_c$ and $F_g$ respectively. 
Also, $\mathbf{\bar{E[P]}}(\sigma,F_{c},F_{g})$ is the normalized mean power consumed by the computation platform.
Finally, $\alpha(t) \in [0,1]$ is a time-varying weight that decides how much to weigh throughput versus performance at time step $t$. 
Note, higher $\alpha(t)$ is, more throughput is weighed. 
This varying $\alpha(t)$ results in different schedules and CPU-GPU frequencies for different times. It is worth noting that for a particular value of $\alpha(t)$, the optimal solution to Eq. \ref{eq:cost_runtime} can be computed by composing the cost function from the profiled data and doing a search across parameters. In order to tie this to control performance, we can make $\alpha(t)$ a function of some feedback that reflects control performance, e.g. vanishing point abscissa or middle point abscissa or both. The function $\alpha(t)$ can now be composed such that as the vanishing point/middle point abscissas take on high values, $\alpha(t)$ also increases, resulting in a lower delay at the cost of more computation power. On the other hand, when the closed loop system is near or at steady state (vanishing point/middle point abscissas are small), we can trade-off computation delay (higher) in order to lower the computation power consumption. 