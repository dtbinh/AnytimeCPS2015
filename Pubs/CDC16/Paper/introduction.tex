\section{Introduction}
\label{sec:intro}

In this paper we are concerned with the problem of controlling nonlinear dynamical systems $S$ of the form $\dot{x} = f(x) + G(x)u$ under state and input constraints, and subject to errors in the state estimate.
This problem is formulated as
\begin{eqnarray}
	\label{eq:generic NLMPC}
	\min_{\textbf{x},\textbf{u}} &\;& l(\textbf{x}, \textbf{u}) \\
	\text{s.t. } \dot{x}&=&f(x) + G(x)u \nonumber \\
	x&\in& X, u \in U  \nonumber
\end{eqnarray}
where $l(\textbf{x}, \textbf{u})$ is a cost function whose minimization over the state and input trajectories $\textbf{x}$ and $\textbf{u}$ ensures stability of the system. 
Sets $X \subset \Re^{\dimX}$ and $ U \subset \Re^{\dimU}$ encode constraints on the state (e.g., safety) and the input.
The input $u = u(\hx)$ is a function of a state estimate that in general differs from the true state of the system.

Whereas Model Predictive Control (MPC) is a widely used control technique for solving the above problem for linear systems, its application to nonlinear systems involves the repeated solution of generally non-quadratic, non-convex optimizations.
Various approaches for solving (or approximately solving) the optimizations and their trade-offs are reviewed in \cite{Cannon04_EfficientMPC}.
Another approach is to first \emph{feedback linearize} the system $S$ \cite{khalil}: namely, the applied control $u = u(x,v)$ is designed in such a way that the resulting closed-loop dynamics $S_{fl}$ are now \emph{linear}:
	\[S_{fl}: \dot{z} = Az + Bv\]
The input $v$ to the linearized dynamics can now be computed so as to optimize system performance and ensure stability.
Since some control authority has been sacrificed to linearize the dynamics, the solution to the linear MPC problem is of course sub-optimal relative to the exact solution to the nonlinear MPC problem.
The state $z$ of the linearized system $S_{fl}$ is related to the state $x$ of the nonlinear system via a (system-specific) function $T$: $z=T(x)$.

Previous work on nonlinear MPC with feedback linearization assumed the state $x(t)$ is perfectly known to the controller at any moment in time \cite{SimonLG13_MPC}.
In practice, only a state estimate $\hx(t)$ is available, and $\hx(t) \neq x(t)$.
Thus a controller designed to work optimally when operating on the true state $x$ is in general sub-optimal when operating on $\hx$ (any may even lead to instability).
Robust MPC (RMPC) has been investigated as a way of handling state estimation errors for linear \cite{RichardsH05_RMPC} and nonlinear systems \cite{tube,relaxed}, but not via feedback linearization. In particular, for non-linear systems, \cite{tube} develops a non-linear MPC with tube like constraints for robust feasibility, but involves solving two (non-convex) optimal control problems. In \cite{relaxed}, the authors solve a non-linear Robust MPC through a bilevel optimization that involves solving a non-linear, non-smooth optimization which is challenging. The paper also guarantees a weaker form of recursive feasibility than the ones proved for Robust linear MPC in \cite{RichardsH05_RMPC} and in this work. In \cite{Zhao20141335} the authors use the Robust MPC of \cite{RichardsH05_RMPC} and feedback linearization for control of a quadrotor; but the MPC is formulated with the linearized dynamics of the quadrotor under hover and the feedback linearization is used for the low level controller. This differs significantly from our approach where we formulate the Robust MPC on the feedback linearized dynamics directly, and not on the dynamics obtained via Jacobian linearization of the non-linear system.
%\todo[inline]{litt search}
Existing work on MPC via feedback linearization and input/state constraints has also assumed that either $T$ is the identity, which simplifies the subsequent stability and performance analysis \cite{SimonLG13_MPC}, or in the case of uncertainties (in the parameters), there are only input constraints and no state constraints \cite{parameter}.
%\todo[inline]{litt search}
In general, a non-identity $T$ is problematic when the state is not perfectly known, since the state estimation error $e = \hx-x$ maps to the linearized dynamics via $T$ in non-trivial ways greatly complicating the analysis.
In particular, the error bounds for the state estimate in $z$-space now depend on the current nonlinear state $x$.
One of the complications introduced by feedback linearization is that the bounds on the input ($u \in U$) may become a non-convex state-dependent constraint on the input $v$ to $S_{fl}$: 
$V = \{\ua{v}(x, U) \leq v \leq \oa{v}(x,U)\}$.
In \cite{SimonLG13_MPC} forward reachability is used to provide inner convex approximations to the input set $V$.
A non-identity $T$ increases the computational burden since the reachability calculation must happen on the non-linear system (with an identity $T$, it can happen on the linear system).

\emph{Contributions}: In this paper we develop a feedback linearization solution to the above control problem, with state estimation errors and non-identity $T$.
The resulting linear control problem is solved by Robust MPC with time-varying constraint sets.
We propose two ways for handling the nonlinear reachability calculation: one is by running an inaccurate but fast nonlinear reachability tool over short horizons, and the other is by running reachability on the linear system and inverting the mapping $T$ in a conservative manner.

The paper is organized as follows,??

